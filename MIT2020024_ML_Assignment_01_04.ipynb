{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT2020024_ML_Assignment_01_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcHQYppsPNJAo8CAd/BmvY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pksj/IIITA_2nd_SEM/blob/main/MIT2020024_ML_Assignment_01_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfwVA1BRoICf"
      },
      "source": [
        "#4.(a) Implement two loss function (given in Figure 1) in logistic regression Generate and report 2 plots one for each cost function. In each plot, you need show two curves: one for the training set and one for the validation set. Run your code several times and observe if the results change. If they do, how would you choose the best parameter settings?\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht7tkYi2y1fR"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# just to get mnist dataset\r\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSdC9pLqqnOv"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CMTicdJqlYf"
      },
      "source": [
        "breast_cancer_data = load_breast_cancer()\r\n",
        "\r\n",
        "features = breast_cancer_data.data\r\n",
        "labels = breast_cancer_data.target\r\n",
        "\r\n",
        "\r\n",
        "# Normalizatin\r\n",
        "\r\n",
        "for i in range(30):                               # number of attributes = 30\r\n",
        "    features[:,i] = (features[:,i] - min(features[:,i])) / (max(features[:,i]) - min(features[:,i]))\r\n",
        "    \r\n",
        "\r\n",
        "training_features = features[:400]\r\n",
        "training_labels = labels[:400]\r\n",
        "\r\n",
        "validation_features = features[400:]\r\n",
        "validation_labels = labels[400:]\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvAEcYooNmJ6"
      },
      "source": [
        "**Hypothesis Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Nbwh65LCTT"
      },
      "source": [
        "def hypothesis(x_data, theta, bias):\r\n",
        "\r\n",
        "    z = np.dot(x_data, theta) + bias\r\n",
        "    # print(\"z\",z)\r\n",
        "    \r\n",
        "    return 1.0  / ( 1.0 +  np.exp(-z))\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikCoGEn1Ng6P"
      },
      "source": [
        "##Loss Function 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Uru5md0EDs"
      },
      "source": [
        "learning_rate = 0.1\r\n",
        "iterations = 1000\r\n",
        "bias = 0.0\r\n",
        "theta_1 = np.zeros( len(training_features[0]))         # there are 30 features"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiS0Op54Jvll"
      },
      "source": [
        "def loss_function_1(x_data, y_data, theta_1, bias):\r\n",
        "\r\n",
        "    cost = 0\r\n",
        "\r\n",
        "    for x , y in zip(x_data, y_data):\r\n",
        "\r\n",
        "        h = hypothesis(x,theta_1, bias)\r\n",
        "\r\n",
        "        cost += (- y) * (np.log(h)) - (1 - y ) * (np.log(1- h))\r\n",
        "    \r\n",
        "    cost = cost / len(x_data)\r\n",
        "    \r\n",
        "    return cost"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z35z2PzSygNu"
      },
      "source": [
        "**Train function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lf3Rb4qzrPx"
      },
      "source": [
        "def train_1(iterations, learning_rate, x_data, y_data, theta_1,bias):\r\n",
        "\r\n",
        "    cost_history = []\r\n",
        "    m = len(x_data)\r\n",
        "    # print(\"m =\", m)\r\n",
        "\r\n",
        "    for i in range(iterations):\r\n",
        "        \r\n",
        "        h = hypothesis(x_data, theta_1, bias)\r\n",
        "        \r\n",
        "        dj_by_dw = ( 1 / m ) * np.dot(x_data.T, h - y_data)\r\n",
        "        dj_by_db = ( 1 / m ) * np.sum(h - y_data)\r\n",
        "        \r\n",
        "        theta_1 = theta_1 - dj_by_dw * learning_rate\r\n",
        "        bias = bias - dj_by_db * learning_rate\r\n",
        "        \r\n",
        "        cost_history.append(loss_function_1(x_data, y_data, theta_1,bias))\r\n",
        "        \r\n",
        "    return theta_1, bias, cost_history"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RitAD2KXyTRU"
      },
      "source": [
        "epochs = np.arange(1, 50)\r\n",
        "\r\n",
        "training_efficiency = []\r\n",
        "validation_efficiency = []\r\n",
        "\r\n",
        "for epoch in epochs:\r\n",
        "\r\n",
        "    cosy_history = []\r\n",
        "\r\n",
        "    theta_1, bias, cost_history = train_1(iterations, learning_rate, training_features, training_labels, theta_1, bias)\r\n",
        "    # print(theta_1)\r\n",
        "    correct_prediction = 0\r\n",
        "\r\n",
        "    for x, y in zip(training_features, training_labels):\r\n",
        "            \r\n",
        "        prediction = hypothesis(x, theta_1, bias)\r\n",
        "\r\n",
        "        #print(prediction, y)\r\n",
        "        if (( prediction >= 0.5 and y == 1 ) or ( prediction < 0.5 and y == 0 )):\r\n",
        "            correct_prediction += 1\r\n",
        "\r\n",
        "    training_efficiency.append(correct_prediction * 100 / len(training_features))\r\n",
        "\r\n",
        "    plt.plot()\r\n",
        "\r\n",
        "\r\n",
        "    correct_prediction = 0\r\n",
        "\r\n",
        "    for x, y in zip(validation_features, validation_labels):\r\n",
        "            \r\n",
        "        prediction = hypothesis(x, theta_1, bias)\r\n",
        "\r\n",
        "        #print(prediction, y)\r\n",
        "        if (( prediction >= 0.5 and y == 1 ) or ( prediction < 0.5 and y == 0 )):\r\n",
        "            correct_prediction += 1\r\n",
        "\r\n",
        "\r\n",
        "    validation_efficiency.append(correct_prediction * 100 / len(validation_features))\r\n",
        "\r\n",
        "\r\n",
        "plt.plot(epochs, training_efficiency)\r\n",
        "plt.plot(epochs, validation_efficiency)\r\n",
        "plt.xlabel(\"Epochs\")\r\n",
        "plt.xlabel(\"Efficiency\")\r\n",
        "plt.legend(['trainig_efficiency', 'validation_efficiency'])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHkTvEmwPXr8"
      },
      "source": [
        "##Loss Function 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e7absuaUXRc"
      },
      "source": [
        "learning_rate = 0.1\r\n",
        "iterations = 1000\r\n",
        "bias = 0.0\r\n",
        "theta_2 = np.zeros( len(training_features[0]))         # there are 30 features\r\n",
        "lambda_value = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbvCNhiAPclA"
      },
      "source": [
        "def loss_function_2(x_data, y_data, theta_2, bias, lambda_val):\r\n",
        "\r\n",
        "    cost = 0\r\n",
        "\r\n",
        "    for x , y in zip(x_data, y_data):\r\n",
        "\r\n",
        "        h = hypothesis(x,theta_2, bias)\r\n",
        "\r\n",
        "        cost += (- y) * (np.log(h)) - (1 - y ) * (np.log(1- h)) \r\n",
        "    \r\n",
        "    cost += (lambda_value / 2) * sum([ t * t for t in theta_2 ])    # regularization part\r\n",
        "    \r\n",
        "    cost = cost / len(x_data)\r\n",
        "    \r\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uC6PpoCQWcr"
      },
      "source": [
        "**Train function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z27MqB61QWcs"
      },
      "source": [
        "def train_2(iterations, learning_rate, x_data, y_data, theta_2,bias, lambda_value):\r\n",
        "\r\n",
        "    cost_history = []\r\n",
        "    m = len(x_data)\r\n",
        "    # print(\"m =\", m)\r\n",
        "\r\n",
        "    for i in range(iterations):\r\n",
        "        \r\n",
        "        h = hypothesis(x_data, theta_2, bias)\r\n",
        "        \r\n",
        "        dj_by_dw = ( 1 / m ) * np.add( np.dot(x_data.T, h - y_data) , lambda_value * theta_2 )\r\n",
        "        dj_by_db = ( 1 / m ) * np.sum(h - y_data)\r\n",
        "        \r\n",
        "        theta_2 = theta_2 - dj_by_dw * learning_rate\r\n",
        "        bias = bias - dj_by_db * learning_rate\r\n",
        "        \r\n",
        "        cost_history.append(loss_function_2(x_data, y_data, theta_2,bias, lambda_value))\r\n",
        "        \r\n",
        "    return theta_2, bias, cost_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYL6nkX4RKE9"
      },
      "source": [
        "**Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S36ZGekRKE9"
      },
      "source": [
        "epochs = np.arange(10, 101, 1);\r\n",
        "\r\n",
        "training_efficiency = []\r\n",
        "validation_efficiency = []\r\n",
        "\r\n",
        "for epoch in epochs:\r\n",
        "\r\n",
        "    cosy_history = []\r\n",
        "\r\n",
        "    theta_2, bias, cost_history = train_2(iterations, learning_rate, training_features, training_labels, theta_2, bias)\r\n",
        "    # print(theta_1)\r\n",
        "    correct_prediction = 0\r\n",
        "\r\n",
        "    for x, y in zip(training_features, training_labels):\r\n",
        "            \r\n",
        "        prediction = hypothesis(x, theta_2, bias)\r\n",
        "\r\n",
        "        #print(prediction, y)\r\n",
        "        if (( prediction >= 0.5 and y == 1 ) or ( prediction < 0.5 and y == 0 )):\r\n",
        "            correct_prediction += 1\r\n",
        "\r\n",
        "    training_efficiency.append(correct_prediction * 100 / len(training_features))\r\n",
        "\r\n",
        "    plt.plot()\r\n",
        "\r\n",
        "\r\n",
        "    correct_prediction = 0\r\n",
        "\r\n",
        "    for x, y in zip(validation_features, validation_labels):\r\n",
        "            \r\n",
        "        prediction = hypothesis(x, theta_2, bias)\r\n",
        "\r\n",
        "        #print(prediction, y)\r\n",
        "        if (( prediction >= 0.5 and y == 1 ) or ( prediction < 0.5 and y == 0 )):\r\n",
        "            correct_prediction += 1\r\n",
        "\r\n",
        "\r\n",
        "    validation_efficiency.append(correct_prediction * 100 / len(validation_features))\r\n",
        "\r\n",
        "\r\n",
        "plt.plot(epochs, training_efficiency)\r\n",
        "plt.plot(epochs, validation_efficiency)\r\n",
        "plt.xlabel(\"Epochs\")\r\n",
        "plt.xlabel(\"Efficiency\")\r\n",
        "plt.legend(['trainig_efficiency', 'validation_efficiency'])\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3WjncG65vCP"
      },
      "source": [
        "##(b) Generate graph with different value of lambda = 0; 0.001; 0.01; 0.1; 1.0 with respect to learning rate, number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-OsJXgm59HC"
      },
      "source": [
        "lambda_values = [0,0.001,0.01,0.1,1]\r\n",
        "\r\n",
        "training_accuracies = []\r\n",
        "validation_accuracies = []\r\n",
        "\r\n",
        "for lambda_value in lambda_values:\r\n",
        "\r\n",
        "    cosy_history = []\r\n",
        "\r\n",
        "    theta_2, bias, cost_history = train_2(iterations, learning_rate, training_features, training_labels, theta_2, bias, lambda_value)\r\n",
        "\r\n",
        "    correct_prediction = 0\r\n",
        "\r\n",
        "    for x, y in zip(training_features, training_labels):\r\n",
        "            \r\n",
        "        prediction = hypothesis(x, theta_2, bias)\r\n",
        "\r\n",
        "        #print(prediction, y)\r\n",
        "        if (( prediction >= 0.5 and y == 1 ) or ( prediction < 0.5 and y == 0 )):\r\n",
        "            correct_prediction += 1\r\n",
        "\r\n",
        "\r\n",
        "    training_accuracies.append(correct_prediction * 100 / len(training_features))\r\n",
        "\r\n",
        "\r\n",
        "    correct_prediction = 0\r\n",
        "\r\n",
        "    for x, y in zip(validation_features, validation_labels):\r\n",
        "            \r\n",
        "        prediction = hypothesis(x, theta_2, bias)\r\n",
        "\r\n",
        "        #print(prediction, y)\r\n",
        "        if (( prediction >= 0.5 and y == 1 ) or ( prediction < 0.5 and y == 0 )):\r\n",
        "            correct_prediction += 1\r\n",
        "\r\n",
        "\r\n",
        "    validation_accuracies.append(correct_prediction * 100 / len(validation_features))\r\n",
        "\r\n",
        "\r\n",
        "plt.figure(figsize=(8,4))\r\n",
        "plt.plot(lambda_values,training_accuracies)\r\n",
        "plt.xlabel(\"lamda value\")\r\n",
        "plt.ylabel(\"training accuracy\")\r\n",
        "plt.show()\r\n",
        "plt.figure(figsize=(8,4))\r\n",
        "plt.plot(lambda_values,validation_accuracies)\r\n",
        "plt.xlabel(\"lamda value\")\r\n",
        "plt.ylabel(\"validation accuracy\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}